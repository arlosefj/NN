{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 双层网络，最基本网络结构，批量梯度下降优化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 导入库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import _pickle as cPickle\n",
    "import gzip\n",
    "from scipy import ndimage\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 载入图像相关函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    \"\"\"Return the MNIST data as a tuple containing the training data,\n",
    "    the validation data, and the test data.\n",
    "    The ``training_data`` is returned as a tuple with two entries.\n",
    "    The first entry contains the actual training images.  This is a\n",
    "    numpy ndarray with 50,000 entries.  Each entry is, in turn, a\n",
    "    numpy ndarray with 784 values, representing the 28 * 28 = 784\n",
    "    pixels in a single MNIST image.\n",
    "    The second entry in the ``training_data`` tuple is a numpy ndarray\n",
    "    containing 50,000 entries.  Those entries are just the digit\n",
    "    values (0...9) for the corresponding images contained in the first\n",
    "    entry of the tuple.\n",
    "    The ``validation_data`` and ``test_data`` are similar, except\n",
    "    each contains only 10,000 images.\n",
    "    This is a nice data format, but for use in neural networks it's\n",
    "    helpful to modify the format of the ``training_data`` a little.\n",
    "    That's done in the wrapper function ``load_data_wrapper()``, see\n",
    "    below.\n",
    "    \"\"\"\n",
    "    f = gzip.open('../data/mnist.pkl.gz', 'rb')\n",
    "    training_data, validation_data, test_data = cPickle.load(f, encoding='latin1')\n",
    "    f.close()\n",
    "    return (training_data, validation_data, test_data)\n",
    "\n",
    "def load_data_wrapper(augmentation=False):\n",
    "    \"\"\"Return a tuple containing ``(training_data, validation_data,\n",
    "    test_data)``. Based on ``load_data``, but the format is more\n",
    "    convenient for use in our implementation of neural networks.\n",
    "    In particular, ``training_data`` is a list containing 50,000\n",
    "    2-tuples ``(x, y)``.  ``x`` is a 784-dimensional numpy.ndarray\n",
    "    containing the input image.  ``y`` is a 10-dimensional\n",
    "    numpy.ndarray representing the unit vector corresponding to the\n",
    "    correct digit for ``x``.\n",
    "    ``validation_data`` and ``test_data`` are lists containing 10,000\n",
    "    2-tuples ``(x, y)``.  In each case, ``x`` is a 784-dimensional\n",
    "    numpy.ndarry containing the input image, and ``y`` is the\n",
    "    corresponding classification, i.e., the digit values (integers)\n",
    "    corresponding to ``x``.\n",
    "    Obviously, this means we're using slightly different formats for\n",
    "    the training data and the validation / test data.  These formats\n",
    "    turn out to be the most convenient for use in our neural network\n",
    "    code.\"\"\"\n",
    "    tr_d, va_d, te_d = load_data()\n",
    "    training_inputs = [np.reshape(x, (784, 1)) for x in tr_d[0]]\n",
    "    training_results = [vectorized_result(y) for y in tr_d[1]]\n",
    "    if augmentation: training_inputs, training_results = expend_training_data(training_inputs, training_results)\n",
    "    training_data = list(zip(training_inputs, training_results))\n",
    "    validation_inputs = [np.reshape(x, (784, 1)) for x in va_d[0]]\n",
    "    validation_data = list(zip(validation_inputs, va_d[1]))\n",
    "    test_inputs = [np.reshape(x, (784, 1)) for x in te_d[0]]\n",
    "    test_data = list(zip(test_inputs, te_d[1]))\n",
    "    return (training_data, validation_data, test_data)\n",
    "\n",
    "\n",
    "def vectorized_result(j):\n",
    "    \"\"\"Return a 10-dimensional unit vector with a 1.0 in the jth\n",
    "    position and zeroes elsewhere.  This is used to convert a digit\n",
    "    (0...9) into a corresponding desired output from the neural\n",
    "    network.\"\"\"\n",
    "    e = np.zeros((10, 1))\n",
    "    e[j] = 1.0\n",
    "    return e\n",
    "\n",
    "# Augment training data\n",
    "def expend_training_data(images, labels):\n",
    "\n",
    "    expanded_images = []\n",
    "    expanded_labels = []\n",
    "\n",
    "    j = 0 # counter\n",
    "    for x, y in zip(images, labels):\n",
    "        j = j+1\n",
    "        if j%100==0:\n",
    "            print ('expanding data : %03d / %03d' % (j,np.size(images,0)))\n",
    "\n",
    "        # register original data\n",
    "        expanded_images.append(x)\n",
    "        expanded_labels.append(y)\n",
    "\n",
    "        # get a value for the background\n",
    "        # zero is the expected value, but median() is used to estimate background's value\n",
    "        bg_value = np.median(x) # this is regarded as background's value\n",
    "        image = np.reshape(x, (-1, 28))\n",
    "\n",
    "        for i in range(4):\n",
    "\n",
    "            ## Augmentation via Rotation\n",
    "            # rotate the image with random degree\n",
    "            angle = np.random.randint(-15,15,1)\n",
    "            new_img = ndimage.rotate(image,angle,reshape=False, cval=bg_value)\n",
    "\n",
    "            # shift the image with random distance\n",
    "            shift = np.random.randint(-2, 2, 2)\n",
    "            new_img_ = ndimage.shift(new_img,shift, cval=bg_value)\n",
    "\n",
    "            # register new training data\n",
    "            expanded_images.append(np.reshape(new_img_, [784,1]))\n",
    "            expanded_labels.append(y)\n",
    "\n",
    "\n",
    "    return expanded_images, expanded_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 载入数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data, validation_data, test_data = load_data_wrapper()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 训练参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epoch = 30\n",
    "learning_rate = 1\n",
    "batch_size = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 网络结构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_node_input = 784\n",
    "n_node_hidden = 100\n",
    "n_node_output = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 权重与偏置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "W2=np.random.randn(n_node_hidden, n_node_input)\n",
    "b2=np.random.randn(n_node_hidden, 1)\n",
    "\n",
    "W3=np.random.randn(n_node_output, n_node_hidden)\n",
    "b3=np.random.randn(n_node_output, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 激活函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    \"\"\"The sigmoid function.\"\"\"\n",
    "    return 1.0/(1.0+np.exp(-z))\n",
    "\n",
    "def sigmoid_prime(z):\n",
    "    \"\"\"Derivative of the sigmoid function.\"\"\"\n",
    "    return sigmoid(z)*(1-sigmoid(z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 开始训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TRAIN_ERROR] Epoch 00: 26006 / 50000\n",
      "[ TEST_ERROR] Epoch 00:  4343 / 10000\n",
      "[TRAIN_ERROR] Epoch 01: 17132 / 50000\n",
      "[ TEST_ERROR] Epoch 01:  2677 / 10000\n",
      "[TRAIN_ERROR] Epoch 02: 12588 / 50000\n",
      "[ TEST_ERROR] Epoch 02:  2525 / 10000\n",
      "[TRAIN_ERROR] Epoch 03: 12032 / 50000\n",
      "[ TEST_ERROR] Epoch 03:  2470 / 10000\n",
      "[TRAIN_ERROR] Epoch 04: 11708 / 50000\n",
      "[ TEST_ERROR] Epoch 04:  2367 / 10000\n",
      "[TRAIN_ERROR] Epoch 05:  8121 / 50000\n",
      "[ TEST_ERROR] Epoch 05:  1552 / 10000\n",
      "[TRAIN_ERROR] Epoch 06:  7117 / 50000\n",
      "[ TEST_ERROR] Epoch 06:  1492 / 10000\n",
      "[TRAIN_ERROR] Epoch 07:  6910 / 50000\n",
      "[ TEST_ERROR] Epoch 07:  1464 / 10000\n",
      "[TRAIN_ERROR] Epoch 08:  6734 / 50000\n",
      "[ TEST_ERROR] Epoch 08:  1449 / 10000\n",
      "[TRAIN_ERROR] Epoch 09:  6615 / 50000\n",
      "[ TEST_ERROR] Epoch 09:  1437 / 10000\n",
      "[TRAIN_ERROR] Epoch 10:  6489 / 50000\n",
      "[ TEST_ERROR] Epoch 10:  1420 / 10000\n",
      "[TRAIN_ERROR] Epoch 11:  6420 / 50000\n",
      "[ TEST_ERROR] Epoch 11:  1387 / 10000\n",
      "[TRAIN_ERROR] Epoch 12:  6323 / 50000\n",
      "[ TEST_ERROR] Epoch 12:  1374 / 10000\n",
      "[TRAIN_ERROR] Epoch 13:  6245 / 50000\n",
      "[ TEST_ERROR] Epoch 13:  1367 / 10000\n",
      "[TRAIN_ERROR] Epoch 14:  6173 / 50000\n",
      "[ TEST_ERROR] Epoch 14:  1364 / 10000\n",
      "[TRAIN_ERROR] Epoch 15:  6101 / 50000\n",
      "[ TEST_ERROR] Epoch 15:  1341 / 10000\n",
      "[TRAIN_ERROR] Epoch 16:  6060 / 50000\n",
      "[ TEST_ERROR] Epoch 16:  1342 / 10000\n",
      "[TRAIN_ERROR] Epoch 17:  6031 / 50000\n",
      "[ TEST_ERROR] Epoch 17:  1343 / 10000\n",
      "[TRAIN_ERROR] Epoch 18:  6012 / 50000\n",
      "[ TEST_ERROR] Epoch 18:  1330 / 10000\n",
      "[TRAIN_ERROR] Epoch 19:  5937 / 50000\n",
      "[ TEST_ERROR] Epoch 19:  1333 / 10000\n",
      "[TRAIN_ERROR] Epoch 20:  5907 / 50000\n",
      "[ TEST_ERROR] Epoch 20:  1316 / 10000\n",
      "[TRAIN_ERROR] Epoch 21:  5834 / 50000\n",
      "[ TEST_ERROR] Epoch 21:  1299 / 10000\n",
      "[TRAIN_ERROR] Epoch 22:  3245 / 50000\n",
      "[ TEST_ERROR] Epoch 22:   517 / 10000\n",
      "[TRAIN_ERROR] Epoch 23:  1610 / 50000\n",
      "[ TEST_ERROR] Epoch 23:   488 / 10000\n",
      "[TRAIN_ERROR] Epoch 24:  1494 / 50000\n",
      "[ TEST_ERROR] Epoch 24:   472 / 10000\n",
      "[TRAIN_ERROR] Epoch 25:  1415 / 50000\n",
      "[ TEST_ERROR] Epoch 25:   436 / 10000\n",
      "[TRAIN_ERROR] Epoch 26:  1345 / 50000\n",
      "[ TEST_ERROR] Epoch 26:   445 / 10000\n",
      "[TRAIN_ERROR] Epoch 27:  1302 / 50000\n",
      "[ TEST_ERROR] Epoch 27:   435 / 10000\n",
      "[TRAIN_ERROR] Epoch 28:  1248 / 50000\n",
      "[ TEST_ERROR] Epoch 28:   446 / 10000\n",
      "[TRAIN_ERROR] Epoch 29:  1192 / 50000\n",
      "[ TEST_ERROR] Epoch 29:   434 / 10000\n",
      "done!\n"
     ]
    }
   ],
   "source": [
    "test_errors = []\n",
    "training_errors = []\n",
    "n = len(training_data)\n",
    "\n",
    "for j in range(n_epoch):\n",
    "\n",
    "    ## Stochastic Gradient Descent\n",
    "    np.random.shuffle(training_data)\n",
    "\n",
    "    # for each batch\n",
    "    sum_of_training_error = 0\n",
    "    for k in range(0, n, batch_size):\n",
    "        batch = training_data[k:k+batch_size]\n",
    "\n",
    "        # average gradient for samples in a batch\n",
    "        sum_gradient_b3 = 0\n",
    "        sum_gradient_b2 = 0\n",
    "        sum_gradient_W3 = 0\n",
    "        sum_gradient_W2 = 0\n",
    "\n",
    "        # for each sample\n",
    "        for x, y in batch:\n",
    "            ## Feed forward\n",
    "\n",
    "            a1 = x\n",
    "            z2 = np.dot(W2, a1) + b2\n",
    "            a2 = sigmoid(z2)\n",
    "            z3 = np.dot(W3, a2) + b3\n",
    "            a3 = sigmoid(z3)\n",
    "\n",
    "            ## Backpropagation\n",
    "\n",
    "            # Step 1: Error at the output layer [Quadratic Cost]\n",
    "            delta_3 = (a3-y)*sigmoid_prime(z3)\n",
    "            # Step 2: Error relationship between two adjacent layers\n",
    "            delta_2 =  sigmoid_prime(z2)*np.dot(W3.transpose(), delta_3)\n",
    "            # Step 3: Gradient of C in terms of bias\n",
    "            gradient_b3 = delta_3\n",
    "            gradient_b2 = delta_2\n",
    "            # Step 4: Gradient of C in terms of weight\n",
    "            gradient_W3 = np.dot(delta_3, a2.transpose())\n",
    "            gradient_W2 = np.dot(delta_2, a1.transpose())\n",
    "\n",
    "            # update gradients\n",
    "            sum_gradient_b3 += gradient_b3\n",
    "            sum_gradient_b2 += gradient_b2\n",
    "            sum_gradient_W3 += gradient_W3\n",
    "            sum_gradient_W2 += gradient_W2\n",
    "\n",
    "            ## Training Error\n",
    "            sum_of_training_error += int(np.argmax(a3) != np.argmax(y))\n",
    "\n",
    "\n",
    "        # update weights & biases\n",
    "        b3 -= learning_rate * sum_gradient_b3 / batch_size\n",
    "        b2 -= learning_rate * sum_gradient_b2 / batch_size\n",
    "        W3 -= learning_rate * sum_gradient_W3 / batch_size\n",
    "        W2 -= learning_rate * sum_gradient_W2 / batch_size\n",
    "\n",
    "    # Report Training Error\n",
    "    print(\"[TRAIN_ERROR] Epoch %02d: %5d / %05d\" % (j, sum_of_training_error, n))\n",
    "    training_errors.append(np.float(sum_of_training_error) / n)\n",
    "\n",
    "    ### Test\n",
    "    n_test = len(test_data)\n",
    "    sum_of_test_error = 0\n",
    "    for x, y in test_data:\n",
    "        ## Feed forward\n",
    "\n",
    "        a1 = x\n",
    "        z2 = np.dot(W2, a1) + b2\n",
    "        a2 = sigmoid(z2)\n",
    "        z3 = np.dot(W3, a2) + b3\n",
    "        a3 = sigmoid(z3)\n",
    "\n",
    "        ## Test Error\n",
    "        # in test data, label info is a number not one-hot vector as in training data\n",
    "        sum_of_test_error += int(np.argmax(a3) != y)\n",
    "\n",
    "    # Report Test Error\n",
    "    print(\"[ TEST_ERROR] Epoch %02d: %5d / %05d\" % (j, sum_of_test_error, n_test))\n",
    "\n",
    "    test_errors.append(np.float(sum_of_test_error)/n_test)\n",
    "    \n",
    "print(\"done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
